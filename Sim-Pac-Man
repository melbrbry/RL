import numpy as np
import random
import mxnet as mx
from copy import deepcopy

OO = int(1e9)
rows = 7
cols = 18
init = [5,16]
permgrid = [['*','*','*','*','*','*','*','*','*','*','*','*','*','*','*','*','*','*'],
            ['*','$','$','$','$','$','$','$','$','$','$','$','$','$','$','$','$','*'],
            ['*','$','*','$','*','$','*','*','*','*','*','$','*','$','*','*','$','*'],
            ['*','$','*','$','*','$','*','$','$','$','*','$','*','$','$','*','$','*'],
            ['*','$','*','*','*','$','*','$','*','*','*','$','*','$','$','*','$','*'],
            ['*','$','$','$','$','$','$','$','$','$','$','$','*','$','$','*','.','*'],
            ['*','*','*','*','*','*','*','*','*','*','*','*','*','*','*','*','*','*']]
#         0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
grid = [['*','*','*','*','*','*','*','*','*','*','*','*','*','*','*','*','*','*'], # 0
        ['*','.','$','$','$','$','.','$','.','$','$','$','$','$','$','$','$','*'], # 1
        ['*','$','*','$','*','$','*','*','.','*','*','$','*','.','*','*','$','*'], # 2
        ['*','.','.','$','*','.','*','$','$','$','$','$','*','$','$','$','$','*'], # 3
        ['*','$','*','$','*','$','*','.','*','*','*','$','*','$','$','*','$','*'], # 4
        ['*','.','$','$','.','$','$','$','$','$','$','$','$','$','$','.','.','*'], # 5
        ['*','*','*','*','*','*','*','*','*','*','*','*','*','*','*','*','*','*']] # 6
batch_size = 1 # batch size, if one -> SGD
alpha = 0.2
epsilon = 0.05
gamma = 0.8
iterNo = 100
ftNo = 4
actionsNo=4
dx = [0,0,-1,1,0]
dy = [1,-1,0,0,0]
actionName = ['R','L','U','D', 'NoOp']
ghosts = []


# the s is the agent position

def build_network():

    # the input layer
    data = mx.sym.Variable('data')
    data = mx.sym.Flatten(data=data)

    # the 1st hidden fully connected layer
    fc1 = mx.sym.FullyConnected(data=data, num_hidden=128)
    act1 = mx.sym.Activation(data=fc1, act_type='relu')

    # the output layer
    fc2 = mx.sym.FullyConnected(data=act1, num_hidden=1)
    op = mx.sym.LinearRegressionOutput(data=fc2)

    model = mx.model.FeedForward(symbol=op, learning_rate=alpha)

    return model

# returns next state results from executing some action in some state
def execAction(s, a):
    sPrime = [s[0] + dx[a], s[1] + dy[a]]
    return sPrime

# returns a list of all feasible, valid, action in some state
def genFeasibleActions(s):
    a = []
    for i in range(actionsNo):
        if not(s[0]+dx[i]<0 or s[0]+dx[i]>=rows or s[1]+dy[i]<0 or s[1]+dy[i]>=cols):
            if grid[s[0]+dx[i]][s[1]+dy[i]]!='*':
                a.append(i)
    return a

# returns the value of the Q of some state-action pair
def getQ(s, a):
    newInput = []
    for i in range(len(grid)):
        xx = []
        for j in range(len(grid[0])):
            xx.append(ord(grid[i][j]))
        newInput.append(xx)
    newRow = [s[0], s[1], a]
    for x in ghosts:
        newRow.append(x[0])
        newRow.append(x[1])
    for i in range(cols - len(newRow)):
        newRow.append(0)
    newInput.append(newRow)
    newInput = [newInput]
    newInp = mx.nd.array(newInput)
    print newInp.shape
   # it = mx.io.NDArrayIter(to4d(newInput))
    return  model.predict(newInp)

# returns the maximum Q over all valid actions from a given state
def argMaxQa(s):
    retA = 0
    maxQ = -OO
    A = genFeasibleActions(s)
    for a in A:
        if getQ(s, a)>maxQ:
            maxQ = getQ(s, a)
            retA = a
    return retA

# returns an action according to epsilon-greedy
def chooseAction(s):
    a = genFeasibleActions(s)
    r = random.random()
    if r < epsilon:
        #print("Exploration")
        return random.choice(a)
    else:
        #print("Exploitation")
        return argMaxQa(s)

# returns the reward of some some state-action-nextState
def getReward(s, a, sPrime):
    if grid[sPrime[0]][sPrime[1]]=='$':
        return 1
    else:
        return 0

def to4d(inputs):
    inp = mx.nd.array(inputs)
    print inp.shape
    return inp.reshape((inp.shape[0], 1, rows+1, cols))

# updates the guess of the weights
def update_network(inputs, targets):
    targets = mx.nd.array(targets)
    train_iter = mx.io.NDArrayIter(to4d(inputs), targets, batch_size, shuffle=True)
    model.fit(X=train_iter)

# move the ghosts randomly
def moveGhosts():
    global ghosts
    for i in range(len(ghosts)):
        A = genFeasibleActions(ghosts[i])
        a = random.choice(A)
        ghosts[i][0] += dx[a]
        ghosts[i][1] += dy[a]

# counts the current number of the biscuits of the grid
def cntBiscuits():
    cnt = 0
    for i in range(rows):
        for j in range(cols):
            if grid[i][j]=='$':
                cnt+=1
    return cnt

# executes the Q learning
def qLearning():
    err = 0
    avgReward = 0
    global alpha
    global epsilon
    global grid
    global ghosts
    cnt = 0
    targets = []
    inputs = []
    for it in range(iterNo):
        #alpha = max(alpha-0.0008, 0.00)
        #epsilon = max(epsilon-0.0002, 0.00)
        ghosts = [[3,3], [3,3]] #  2 ghosts
        #         0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
        grid = [['*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*'],  # 0
                ['*', '.', '$', '$', '$', '$', '.', '$', '.', '$', '$', '$', '$', '$', '$', '$', '$', '*'],  # 1
                ['*', '$', '*', '$', '*', '$', '*', '*', '.', '*', '*', '$', '$', '.', '*', '*', '$', '*'],  # 2
                ['*', '.', '.', '$', '*', '.', '*', '$', '$', '$', '$', '$', '*', '$', '$', '$', '$', '*'],  # 3
                ['*', '$', '*', '$', '*', '$', '*', '.', '*', '*', '*', '$', '*', '$', '$', '*', '$', '*'],  # 4
                ['*', '.', '$', '$', '.', '$', '$', '$', '$', '$', '$', '$', '$', '$', '$', '.', '.', '*'],  # 5
                ['*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*', '*']]  # 6
        bisc = cntBiscuits() #initial number of biscuits
        s = []
        s.append(init[0])
        s.append(init[1])
        totalReward = 0
        while not (bisc==0): #loop until no biscuits are remaining
            print "ssss"
            cnt += 1
            xxx = 1
            a = chooseAction(s)
            #if it+1 == 100:
                #print("State:",s)
                #print("Ghosts:", end='')
                #print(ghosts)
                #print("Action:", actionName[a])
                #print(weights)
            sPrime = execAction(s, a)

            # The next few lines check if the agent collides with a ghost
            v = []
            for i in range(len(ghosts)):
                if ghosts[i]==sPrime:
                    v.append(i)
            moveGhosts()
            #if it + 1 == 100:
                #print("Ghosts After:",ghosts)
            for i in range(len(ghosts)):
                if i in v and ghosts[i]==s:
                    xxx = 0

            # if the agent collides with a ghost, end the episode
            #print(" ,sPrime:", sPrime, end='')
            r = getReward(s, a, sPrime)
            if not xxx or (sPrime in ghosts):
                r = -10
            #print(" ,Reward:", r)
            totalReward += r
            qsa = getQ(s, a)

            if grid[sPrime[0]][sPrime[1]]=='$':
                #if it + 1 == iterNo:
                    #print("Ate a biscuit")
                grid[sPrime[0]][sPrime[1]] = '.'
                bisc-=1
            aPMax = argMaxQa(sPrime)
            qsaPMax = getQ(sPrime, aPMax)


            # Collecting samples
            # input: batch of collected states (grid+ghosts+state+action)
            # target: r + gamma * qsaPMax
            newInput = []
            for i in range(len(grid)):
                xx = []
                for j in range(len(grid[0])):
                    xx.append(ord(grid[i][j]))
                newInput.append(xx)
            newRow = [s[0], s[1], a]
            for x in ghosts:
                newRow.append(x[0])
                newRow.append(x[1])
            for i in range(cols-len(newRow)):
                newRow.append(0)
            newInput.append(newRow)
            inputs.append(newInput)
            targets.append(r+gamma*qsaPMax)

            # update the network
            if (cnt)%batch_size==0:
                update_network(inputs, targets)
                inputs=[]
                targets=[]

            if not xxx or (sPrime in ghosts):
                #print("Collided with a ghost")
                break
            #print(w)
            s[0] = sPrime[0]
            s[1] = sPrime[1]

        if bisc!=0:
            err+=1
        #print("Iteration No.:", it+1)
        #print("weights: ", end='')
        #print(weights)
        #print("Bisuits remaining: ", bisc)
        #print("Total Reward:", totalReward)
        avgReward += totalReward
    avgReward /= iterNo
    print("Average Reward:", avgReward)
    print("Success times:", iterNo-err)
    print("Success rate:", (iterNo-err)/iterNo)

model = build_network()

print "network build is OK"

qLearning()
